Apache Spark:
No. 1 Big Data tool these days to process datasets and perform analysis.
It is even considered to be a "Hadoop Killer".
Spark has advantages over Hadoop.
Spark is written in Scala.

Hadoop, which uses MapReduce (MR) for processing, performs of lot Disk I/O.
Spark completely avoids Disk I/O. It does processing in-memory! 
In-memory cluster computing is the main feature of Spark.
Because of this, Spark is considered ~100 times faster than Hadoop MR.

Spark distributes processing across nodes, which is why it fault-tolerant and highly scalable.
With Spark, you can use AWS S3 instead of HDFS for storage.

Spark is NOT a modified version of Hadoop.

Features:
Speed: due to in-memory cluster computing.
Supports multiple languages: Scala, Java, Python. There are some features of Spark that do not work with Python.
Advanced Analytics: Has it's own "map" & "reduce". Also supports SQL queries, Streaming data, Machine Learning (ML) and Graph algorithms.

Hadoop vs Spark:
----------------
					Hadoop MapReduce			Spark
					------------------------------------------------------
Storage				Disk only.					In-memory or disk.
Operations			Map and Reduce.				Map, Reduce, Join, Group etc.
Execution Model		Batch						Batch, streaming, interactive.
Programming env.	Java, Python Scala.			Java, Scala and Python (with limitations in Python). Also supports R.

Spark Architecture:
Driver program listens for incoming connections, accepts them and gives them to the Worker Node for execution.
Driver program co-ordinates with the Executor on each node.
Each Worker Node has 1 Executor, some Cache storage and tasks that are executed (and monitored by the Executor).
You can configure the amount of memory that will used by the executor. You can do this by setting a property called spark.executor.memory.

How to determine no. of executors for a job:
Nodes = 10.
Each node has 16 cores (-1 for OS).
Each node has 61GB RAM (-1 for OS).
A general rule of thumb is an executor can run 5 concurrent tasks in parallel.
No. of cores available is 15.
No. of executors = no. of cores available / concurrent tasks = 15 / 5 = 3.

= no. of nodes * no. of executors on each node
= 10 * 3
= 30 executors per Spark Job.

RDD - Resilient Distributed Datasets:
--------------------------------------
Fundamental data structure in Spark.
It is an immutable distributed collection of objects.
Each dataset in RDD is divided into logical partitions, which may be computed on different nodes of the cluster.
RDDs can contain objects of Python, Java, Scala. Even objects of classes you create in your code.
Resilient because they are able to withstand failure / recover quickly from failure. They are fault-tolerant.

There are 2 ways in which you can create an RDD:
1. parallelizing existing collections
2. Referencing a dataset read from an external storage system like HDFS, AWS S3, HBase etc.

Data Sharing in MapReduce is slow.
	because MR does a lot of Disk i/o.
	
The RDDs with the data are processed in-memory by Spark when you run queries etc. on the RDDs.
Whereas MapReduce would have used disk to share data between iterations.

Two ways Data Sharing works with Spark RDD:
1. Iterative Operations on Spark RDD.
	Store intermediate results in distributed memory which is used by different iterations.
	
2. Intreactive Operations on Spark RDD.
	Different queries are executed on the same set of data repeatedly.
	
It may happen that when data is stored in distributed memory, the node(s) run out of memory. Then in such cases, the intermediate results are stored on disk.

Spark Core Programming:
Open the spark-shell
read a file from the local disk on the VM:
val file = sc.textFile("file:///home/maria_dev/SparkSamples/sparkscala/src/main/scala/example/twinkle.txt")

if we are reading from HDFS:
val file = sc.textFile("twinkle.txt")		// Path would be /user/maria_dev/twinkle.txt

We have to count no. of words in the file.
val wordCount = file.flatMap(line => line.split(" ")).map(word => (word,1)).reduceByKey(_+_)

Steps are:
read each line
split into words separated by the " "
initiaze each word with 1. For e.g.; (Twinkle, 1) where the word is the "key" and the number, 1 in this case, is the value.
finally, reduce the keys by adding values of similar keys.

For e.g.;
twinkle twinkle little star
(twinkle,1)
(twinkle,1)
(litte,1)
(star,1)

Save the resulting RDD to HDFS:
wordCount.saveAsTextFile("twinkle-output")		// path is /user/maria_dev/twinkle-output

It creates a folder on HDFS with the given name and that folder will have 1 or more files with result as follows:
(diamond,1)
(are,1)
(high,1)
(how,1)
(twinkle,4)
(star,2)
(what,1)
(so,1)
(little,2)
(world,1)
(wonder,1)
(up,1)
(you,1)
(a,1)
(above,1)
(I,1)
(in,1)
(like,1)
(the,2)
(sky,1)

Save the resulting RDD to local disk on VM:
wordCount.saveAsTextFile("file:///home/maria_dev/tmp/twinkle-output")		// path is /user/maria_dev/twinkle-output

This will create a folder on your local disk on VM at /home/maria_dev/tmp/twinkle-output, which will have 1 or more files with the result as mentioned above.

We did 2 things here:
1) Transformation.
2) Action.

RDD Transformations:
They return a new RDD and also allows you to create dependencies between RDDs, thus creating a dependency chain by creating one RDD from another.
This is known as RDD Lineage.

Some transformation functions are:
flatMap()
map()
reduceByKey()

RDD Transformations in Spark are "LAZY"!!!!
Meaning, in the following code:
val file = sc.textFile("twinkle.txt")
val wordCount = file.flatMap(line => line.split(" ")).map(word => (word,1)).reduceByKey(_+_)
wordCount.saveAsTextFile("twinkle-output")

The file is not processed (the flatMap, Map and reduceByKey operations are not performed) when that particular is executed!
It is executed / processed ONLY when you perform an "ACTION" on the RDD!!!
So, when you exeucted .saveAsTextFile(), that is when the RDD was processed, that is when the file was actually read.

To determine no. of partitions of an RDD use getNumPartitions:
println(s"Initial Partition count: ${file.getNumPartitions}")					// prints 2.

To change the no., of partitions, use repartition(n):
val repartitionedRDD = file.repartition(4)			// Transformation.
println(s"Initial Partition count: ${repartitionedRDD.getNumPartitions}")		// prints 4.

Some Transformations and Actions:
.collect()	: Action: Returns/Collects the data in the RDD.
	file.collect()
	wordCount.collect()
.foreach()	: Action: iterate the data and process.
	file.collect().foreach(println)
	wordCount.collect().foreach(println)
.flatMap: Transformation.
	val rdd2 = file.flatMap(f => f.split(" "))
	rdd2.foreach(f => println(f))
.map: Transformation.
	val rdd3 = rdd2.map(m => (m,1))
	rdd3.foreach(println)
.filter(): Transformation.
	val rdd4 = rdd3.filter(f => f._1.startsWith("a"))
	rdd4.foreach(println)
.reduceByKey: Transformation.
	val rdd5 = rdd3.reduceByKey(_ + _)
	rdd5.foreach(println)
.sortByKey: Transformation
	// Swap the K-V and then sort.
	val rdd6 = rdd5.map(w => (w._2, w._1)).sortByKey()
	rdd6.foreach(println)
.count:	Action.
	println(s"There are ${rdd6.count()} words...")
.first: Action.
	val firstRecRDD = rdd6.first()
	println(s"First record: ${firstRecRDD._1} : ${firstRecRDD._2}")
.max: Action
	val maxRDD = rdd6.max()
	println(s"Max record: ${maxRDD._1} : ${maxRDD._2}")
.reduce: Action.
	val totalWordCountRDD = rdd6.reduce((a,b) => (a._1 + b._1, a._2))
	println(s"${totalWordCountRDD._1}")
.take: Action.
	val takeRDD = rdd6.take(3)
	takeRDD.foreach(println)
	takeRDD.foreach(f => { println(s"Key: ${f._1}, Value: ${f._2}") } )
.collect: Action.	
	val collectRDD = rdd6.collect()
	collectRDD.foreach(f => { println(s"Key: ${f._1}, Value: ${f._2}") } )
.saveAsTextFile: Action.
	rdd6.saveAsTextFile("wordCount")					// Creates a folder on HDFS.
	rdd6.saveAsTextFile("file:///tmp/wordCount")		// Creates a folder on local machine (VM).
	
Creating RDDS using parallelizing technique:
--------------------------------------------
val parallelRDD = sc.parallelize(Seq(("A", 1), ("B", 2), ("C", 3)))
parallelRDD.collect().foreach(println)
val parallelRDD2 = sc.parallelize(Seq(("B", 4), ("E", 5)))
parallelRDD2.collect().foreach(println)

val cogroupRDD = parallelRDD.cogroup(parallelRDD2)
cogroupRDD.collect().foreach(println)

Spark Basics Overview:
----------------------
RDD - Resilient Distributed Datasets
	Resilient - means able to withstand or recover quickly from failure / difficult conditions.
	Fault-tolerant
	val rdd1 = sc.textFile("....")
	val rdd2 = rdd1.flatMap(...)
	val rdd3 = rdd2.map(...)
	val rdd4 = rdd3.reduceByKey(...)
	
Transformations return a RDD.
RDDs are immutable.
Transformations are lazily evaluated.
	They will be availuated when you perform an Action on the RDD.
	
Some Actions:
rdd1.collect()
rdd2.count()
rdd3.collect().foreach()
rdd4.saveAsTextFile("...")

RDD Lineage: when you create a RDD from another RDD. Basically, create a "lineage".
	For e.g.; created rdd4 from rdd3. rdd3 from rdd2 and rdd2 from rdd1.
	
Spark Deploy Modes:
-------------------
Two Deploy Modes:
1. Local (client).
2. Cluster Mode.

1. Client Mode (Local):
When the driver program runs on the same machine from which the job was submitted.
For e.g.; a single node cluster or you do a remote connection to a server on the cluster and run the Spark job manually either using spark-shell or spark-submit.
The problem with this mode is that if the driver program fails, then the entire job fails.
Not very efficient from a performance perspective (because you are running it locally). You are not making use of the cluster.
This mode will never be used in a production (live) environment.

To run a jonb in client mode:
spark-submit --deploy-mode client ....
For e.g.; spark-submit ./scalahive_2.11-0.1.0-SNAPSHOT.jar  --class example.HiveDemo --deploy-mode client

This mode (client mode) is also the default mode when you start the spark-shell.

2. Cluster mode:
The driver program does not run on the machine from where the job was submitted. It runs on the cluster as a sub-process of the Application Master.
Makes use of the cluster features.
If the driver program crashes or fails, it will automatically be re-instantiated by YARN.

To run a job in cluster mode:
spark-submit --deploy-mode cluster ....
For e.g.; spark-submit ./scalahive_2.11-0.1.0-SNAPSHOT.jar  --class example.HiveDemo --deploy-mode cluster

Here, the application / spark jon will run on the cluster.
Cluster mode is not supported in the interactive shell mode (spark-shell).

Cluster mode is used in a production (live) environment.

Steps to run scala files from spark-shell:
1. Start spark-shell (from the folder where the .scala files are located).
2. :load <scala filename>
Note: In your scala code, comment the lines where you:
	- create the SparkSession and SparkContext objects.
	- specify the pakcage name (at the top).
	- create the object and the main method.
	- and the closing curly-braces (}) for the object and main method (at the end of the code).

Steps to run scala code using spark-submit:
1. Import packages:
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession

2. Create SparkSession:
    val spark:SparkSession = SparkSession.builder()
      .master("local[3]")
      .appName("ajaysingala.com")
      .getOrCreate()
	  
	 here, "local[3]" means run the code locally and use 3 cores. To use all cores, specify "[*]".
	 
3. Create the SparkContext:
    val sc = spark.sparkContext

1. compile your sbt project.
2. create the jar file (run package command in sbt).
3. copy the jar file to your vm using the scp command:
scp -P 2222 .\sparkscala_2.11-0.1.0-SNAPSHOT.jar maria_dev@sandbox-hdp.hortonworks.com:/home/maria_dev

4. Run the job as follows:
spark-submit --deploy-mode client ./sparkscala_2.11-0.1.0-SNAPSHOT.jar  --class sparkbasics.WordCountExample 


SparkSession and SparkContext:
------------------------------
SparkContext (SC):
In earlier versions of Spark (or PySpark), SC was the entry point to Spark programming with RDD and connect to cluster.
With Spark 2.0 SparkSession (SS) has been introduced as the entry point from which you create the SC.
Also, with Spark 2.0, SS became the entry point for programming with DataFrame and DataSet.

SC is used to programmatically create RDD, accumulators and broadcast variables on the cluster.
With Spark 2.0, most of the functionalities in SC are available in SS as well.

In spark-shell, the SparkContext is available by default in the variable named "sc".

SparkSession:
Introduced in Spark 2.0.
Is an entry point to the underlying Spark functionality to programmatically create Spark RDD, DataFrame and DataSet.

In spark-shell, the SparkSession is available by default in the variable named "spark".

SS also includes APIs for different contexts:
- Spark Context
- SQL Context
- Hive Context
- Streaming Context

- Create SparkSession:
    val spark:SparkSession = SparkSession.builder()
      .master("local[3]")
      .appName("ajaysingala.com")
      .getOrCreate()
	  
	 here, "local[3]" means run the code locally and use 3 cores. To use all cores, specify "[*]".
	 
- Create the SparkContext:
    val sc = spark.sparkContext

RDD Lineage a.k.a. RDD Dependency Graph.
Transformations are of 2 types:
- Narrow Transformations.
map() and filter()
All the elements that are required to compute the records in single partition live in the single partition of the parent RDD.
a limited subset of partition is used to calculate the result.
These transformations compute the data that live on a single partition. There will not be any movement of data between partitions to execute narrow transformations.

- Wide Transformations.
All the elements that are required to compute the records in the single partition may live in many partitions of the parent RDD.
These transformations compute data that live on may partitions. There will be data movement between partitions to execute wide transformations.
Since this data movement "shuffles" data between partitions, it is known as shuffle transformations.
reduceByKey(), groupByKey(), repartition()

Shared Variables:
=================
2 types of Shared Variables:
- Broadcast Variables: used to efficiently distribute large datasets across nodes.
- Accumulators: used to aggregate information of a collection.

Broadcast Variables (BVs):
They are cached on each machine (node) rather than making a copy of the data with the tasks.
Spark uses very efficient broadcast algorithms to distribute broadcast variables so as to reduce cross-node communication. Hence it is much faster.
By using BVs, you reduce the number of shuffles and hence increase the performance.

How to create and use BVs:
1. Define a collection:
  val states = Map(("NY","New York"),("CA","California"),("FL","Florida"), ("NSW", "New South Wales"))
  val countries = Map(("USA","United States of America"),("AU","Australia"))

2. Create a BV using spark.sparkContext.broadcast():
  val broadcastStates = spark.sparkContext.broadcast(states)
  val broadcastCountries = spark.sparkContext.broadcast(countries)
  
  OR
  
  val broadcastStates = sc.broadcast(states)
  val broadcastCountries = sc.broadcast(countries)
 
3. Create the source data:
  val data = Seq(("James","Smith","USA","CA"),
    ("Michael","Rose","USA","NY"),
    ("Robert","Williams","USA","CA"),
    ("David","Warner","AU","NSW"),
    ("Maria","Jones","USA","FL")
  )

4. Parallelize the source data:
  val rdd = spark.sparkContext.parallelize(data)  // sc.parallelize(data)

5. Process the data and extract names of states and countries from BVs:
  val rdd2 = rdd.map(f=>{
    val country = f._3
    val state = f._4
    val fullCountry = broadcastCountries.value.get(country).get
    val fullState = broadcastStates.value.get(state).get
    (f._1,f._2,fullCountry,fullState)
  })

6. Print the original source data:
  rdd.foreach(println)

7. Print the data with values extracted from BVs:
  println(rdd2.collect().mkString("\n"))

Accumulators:
Are shared variables used for aggregating data across executors (nodes).
Similar to how MapReduce counters.

Log file: City,State,Product,Amount
Sample data:
-------------------------
Dallas,TX,Erasers,100

Orlando,FL,Pencils,125
Bad data packet
Boston,MA,Markers,0
--------------------------

In the log file, there are some records that are corrupted.
For e.g.; 2nd line is blank. 4th line has some error info. Last line has sales amount as 0 (not possible).

Determine how many blank lines are there in the log file.

var blankLinesCount: Int = 0
sc.textFile("sales.log")
	.foreach { line => if(line.length == 0) blankLinesCount += 1 }

println(s"There are $blankLinesCount blank lines in the file.")

This will print "There are 0 blank lines in the file."
This is because when Spark distributes (ships) this code to every executor, the variable "blankLinesCount" becomes local to that executor.
So, it's updated value is not relayed back to the driver program.
To avoid this, we can make "blankLinesCount" variable as an "Accumulator" such that all the updates from all executors to this variable is relayed back to the driver.

var blankLinesCount: sc.accumulator(0, "Blank Lines")
sc.textFile("sales.log")
	.foreach { line => if(line.length == 0) blankLinesCount += 1 }

println(s"There are $blankLinesCount blank lines in the file.")

Reading data files into RDD:
============================
Read files using the sc.textFile method.
It takes 2 params:
	- 1st is the path to the file(s).
	- 2nd is an optional param to specify no. of partitions.
	
val rddFromFile = spark.sparkContext.textFile("file:///home/maria_dev/SparkSamples/resources/csv/text01.txt")
val rddFromFile = spark.sparkContext.textFile("file:///home/maria_dev/SparkSamples/resources/csv/text01.txt", 3)

Wildcards:
* means 0 or more characters
Text*.txt: Text.txt Text0.txt Text01.txt Text001.text Texts00000001.txt
? means 1 and only 1 character.
Text?.txt: Text0.txt Text1.txt TextA.txt Text_.txt
Text??.txt: Text00.txt Text01.txt Text0A.txt Text_A.txt

Paired RDDs:
============
Key-Value Pair RDDs.
RDDs that have key-value pairs in them.


Transformations:
	-aggregate()
	-aggregateByKey()

These transformations take 3 parameters:
Param1: is the initial value to begin aggregation (which usually is 0).
Param2: (Sequence Operator) the operator (function to perform) to accumulate the results of each partition, and stores the running accumulated result and returns it to the driver.
Param3: (Combined Operator) This operator is used to combine the results of all partitions and give the final result.

AWS EMR:
========
AWS = Amazon Web Service. Cloud platform from Amazon. Cloud Service Provider (CSP).
Other CSPs are Microsoft Azure, Google Cloud Platform (GCP) and many more.
EMR = Elastic MapReduce.
	- The Hadoop Cluster you can setup on AWS.
	- You don't have to install anything. The Hadoop cluster is readily available.
	- You just select you want on the cluster and provision it.
	- AWS will then provision some VMs (EC2 (Elastic Compute Cloud) instances) and have the Hadoop cluster configured on them.
	
Big Data on Cloud:
- on AWS: AWS EMR.
- on Azure: MS Azure HDInsight.
- on GCP: Google DataProc.

Storage is a big decision when hosting Hadoop cluster on cloud.
Options are:
- HDFS
- AWS S3

What is AWS S3:
---------------
S3 is a distributed managed file system provided by AWS.
Similar to HDFS: distributed, fault-tolerant etc.

Why S3 and not HDFS?
When you provision a Hadoop Cluster on AWS EMR and use HDFS for storage, and run a spark job on the cluster.
The job will read an input file from HDFS, process it, write the results back to HDFS.
Remember:
	- on AWS (or for that matter any CSP), you will be charged fo the time the VMs are running. so as long as the VMs (instances) are running, you will be charged of it.
	- After the Spark job has completed and the instances are still running (doing nothing, just idle), you still get charged for it because the instances are running.
Ideally, after the Spark job completes, the cluster is "de-provisioned" or "terminated". This is not same as "Start" and "Stop".
This is so that you do not get charged for idle instances.
But, when the cluster terminates, the instances are not accessible and hence the HDFS storage is also not accessible.
Which means the result generated and saved by the Spark job is lost!!!
What's the solution to this?
	Keep the cluster running and keep paying for idle time!
	Not advisable!
Alternate solution:
	Use AWS S3!
	It does not depend on any VM instance to be idle/ terminated / running etc.
	You still run your Spark job on the AWS EMR (Hadoop) Cluster.
	But use S3 as the storage instead of HDFS.
	Your input file will be on S3.
	And the Spark job will save/store the results on S3 as well.
	So, when the job complets and the cluster is terminated, you still have the data (input and the output) on S3 which is accessible.
	In this way, you do not pay for idle instances.
	S3 is decoupled from the AWS EMR cluster.
	You only pay for the storage used and not the VMs (EC2 instances of the cluster).
	
How to use AWS Cloud:
1. You have to register.
2. There are some services that are free.
3. But there are many that are not free. You have to pay for those. AWS EMR is one of them.
4. You need a credit card to register on AWS.
5. Billing is per month.

AWS S3: IaaS, SaaS
S3 is a distributed managed file system provided by AWS.
It is very similar to any file system where you can organize your files in folders and sub-folders.
On S3, you have to first create a "bucket" and within a bucket, you can then create as many folders and sub-folders as you want.

EC2: Elastic Compute Cloud: IaaS
===========================
When you provision a VM on AWS, you are actually using the EC2 service of AWS.
The VM is known as an "EC2 instance".
So, when we provision an AWS EMR cluster, it will be made of some EC2 instances.

Types of instances in AWS:
- on-demand instance: 
	- Procure when you need them. You provision it based on your requirements and then shut it down or terminate it.
	- Pay for what you use on hourly basis.
	- If you shut down a VM (EC2 instance), you do not pay the "running charges".
	- An instance that has been shut down, can be restarted.
	- But an instance that has been terminated, cannot be restarted.
	- AWS EMR dos not have a "shut down" option. You can only terminate it.
- spot instances:
	- AWS has many instances that remain unused. Lot of free capacity.
	- AWS gives these to users at a highly discounted price. ~90% cheaper compared on-demand.
	- But, AWS can take it back whenever it wants at a 2-minute notice.
	- This is because the spot-instances are based on the "auction" strategy.
	- Used for non-critical application and when you have very low budget.
	- Use it wisely!
- Reserved instances:
	- Commit upfront on the usage of the instance(s).
	- AWS will give discounts for upfront payments and long term commitments.
	- It is cheaper compared to on-demand, provided you use it properly.
	- Mostly used for PRODUCTION environments.
	
Node Types on AWS EMR Spark Cluster:
------------------------------------
1. Master node:
	- Manages the cluster.
	- Single EC2 instance.
2. Core Node:
	- One or more core nodes.
	- Hosts HDFS data and also capable of running tasks (both storage and compute).
3. Task Node:
	- Only compute (you cannot use it for HDFS storage).
	- Can only be used for running tasks.
	- Have as many as you want.
	
Each node is an EC2 instance.

If your application is compute-heavy (requires a lot of processing, less storage), then you can go for more task nodes to adds more computing power to your cluster.
For Taks Nodes, you can also go for spot instances because even if an instance taken away, you are not losing any data.


The Cluster can be of two types:
- Transient cluster.
	- Will auto-terminate after all the steps in a job are completed.
	- a.k.a. Step Execution.
- Long running cluster.
	- when you are doing some R&D, continuous analysis etc.
	- Make sure you terminate the cluster when not in use to avoid recurring charges.
	
Scenario: A reporting Spark Job that needs to run at 12:00 pm daily.
	- Go for Transient cluster.
	- Do the job and terminate the cluster.
	
Creating an AWS EMR Cluster:
1. EC2 Key pair for SSH.
	- Create and download the .ppl file to be used in PuTTY to connect to the EC2 instance.
2. Create an AWS EMR Cluster.
	- 2 Launch modes:
		- Cluster (select this mode).
		- Step Execution.
	- When creating the cluster, status is "Starting".
	- When it is ready for use, status is "Waiting".
	- When terminated, status is "Terminated".

Run a Word Count Spark Job on the Long running cluster:
1. Create the package (jar file) of the scala project in sbt.
2. Create a S3 bucket and folder for the text file to be processed by the WordCount Spark Job.
3. Copy the text file to S3 in the bucket+folder created in step 2.
4. Create a S3 bucket (can use the same bucket) and folder for the jar file (package) created in step 1.
5. Copy the jar file from step 1 to the bucket+folder created in step 4.
6. Using PuTTY, connect to the server (master node of the cluster). 
	Hostname: hadoop@<public dns address of the master node> (The dns can be found on the EMR Cluster's dashboard).
	Port: 22
	In Connection -> SSH -> Auth: Select the .ppk file downloaded earlier (EC2 Key Pair file).
	Select Open.
7. It will fail initially because the port 22 is not open on the master node.
8. From AWS EC2 section, select the master node instance and then the "Security" tab in the bottom pane.
9. Scroll down to "Security Groups" in the Security tab and click on the link there.
10. Select the Inbound Rules tab.
11. Click on Edit Inbound Rules.
12. Scroll down and select Add Rule.
13. Select SSH, Port 22, My IP.
14. Click on Save Rules at the bottom.
15. Try PuTTY again on Step 6.
16. Success!
17. Copy the jar file from s3 to a folder on the instance by executing:
	aws s3 cp <url of the jar on s3> .
	For e.g.; aws s3 cp s3://ajshdoop/jars/wordcount-SNAPSHOT.jar .
18. Run the spark job as:
	spark-submit ./wordcount-SNAPSHOT.jar --class example.WordCount

Spark Architecture:
Cluster Manager can be any one of the following:
- YARN.
- Spark Standalone Mode.
- Mesos.
- Kubernetes.

Driver:
It is a Java process.
This is the process where the main() method of our Scala, Java, Python program runs.
It executes the user code, creates a SparkSession / SparkContext.
SS / SC are responsible to create RDDs, DFs, DS, execute SQL, perform Transformations, Actions etc.
It determines what tasks are to be executed based on the RDD Lineage.
It helps to create a Logical and Physical Plan, which are then scheduled for execution on the worker nodes in co-ordination with the Cluster Manager.

Executor:
Executor resides on the worker node.
They are launched at the start of a Spark Application in co-ordination with the Cluster Manager.
It's responsibility is to run individual tasks and return the results to the driver.
If required, it will cache the data in the worker node.

Cluster Manager:
Spark is dependent on the Cluster Manager to launch executors and also the driver.
When we do a spark-submit, this is the Spark job launch command which helps us connect with the cluster manager, which in turn controls / allocates the necessary resources for the job to be executed on the worker nodes.
It also monitors each worker node.


Spark Cluster settings:
Executors
Memory
Caching

In code:
val spark: SparkSession = SparkSession
.builder()
.master("local[1]")
.appName("AjaySingala.com")
.getOrCreate()

spark.conf.set("spark.executor.instances", 4)
spark.conf.set("spark.executor.cores", 2)

using spark-submit:
spark-submit 
	--executor-memory 16g 
	--executor-cores 2
	--drive-memory 8g
	--num-executors 4
	--deploy-mode cluster
	--class <name of the package.classname to run>
	--master yarn

Caching:
rdd.cache()
rdd.persist()

Types of caching storage level that can be used:
MEMORY_ONLY: 
default. 
RDD is stored as deserialized Java objects in JVM.
If the size of the RDD is > the memory, it may not cache some partition and will recompute them next time whenever needed.
The space used for storage is very high, the CPU computation time is low, the data is stored in memory.
It does not make use of disk.

MEMORY_AND_DISK:
RDD is stored as deserialized Java objects in the JVM.
When the RDD size is > the memory, it stores the excess partition on disk, and retrieves it from disk whenever required.
The level of space used for storage is high, CPU computation time is medium, it makes use of both, in-memory and disk storage.

MEMORY_ONLY_SER:
RDD is stored as serialized Java objects.
It is more space efficient as compared to deserialized objects.
But because it has to do this serialization, there is a overhead on the CPU.
The storage space is low, CPU computation time is high, the data is stored in-memory.
It does not make use of disk.

MEMORY_AND_DISK_SER:
Very similar to MEMORY_AND_DISK and MEMORY_ONLY_SER.
RDD is stored as serialized Java objects.
So any partition that does not fit into memory is dropped on the disk.
Storage space used is low.
Computation is high, makes use of both in-memory and disk storage.

DISK_ONLY:
RDD is stored only on disk.
The storage space used is low, CPU computation time is high, makes use of disk storage.

rdd.cache()
rdd.persist(StorageLevel.MEMORY_ONLY_SER)

To remove the data (Rdd) from cache:
rdd.unpersist()


SparkSQL and DataFrames:
========================
Spark SQL is a Spark module for structured data processing.
You can do analysis on the data using SQL queries.
It internally performs extra optimzations to do the operations and return the result.
Different ways in which you can interact with Spark SQL:
	- using SQL.
	- Dataset API.

Any operation you perform with Spark SQL, it will return DataFrame/Dataset.

Datasets (DS) and DataFrames (DF):
----------------------------------
A Dataset is distributed collection data.
It is a new interface that has been added in Spark 1.6.
Provides benefits of RDDs (strong typing, use lambda functions) along with benefits of SparkSql's optimized execution engine.
You can construct Datasets from JVM objects and then manipulate them using transformations like map, flatMap, filter etc.
This Dataset API is only available in Scala and Java. 
Python does not support the Dataset API.
	Same with R.
Optimization Engine is known as the Catalyst Optimizer.
Can find the schema automatically.
Faster than RDDs but slower than DF.

A Dataframe is a Dataset organized into named columns.
Conceptually, it is equivalent to a table in and RDBMS. R/Python have their Dataframes which are similar.
DF can be constructed from a wide array of sources like: structured data files, tables in Hive, external databases, existing RDDs.
DF API is available in Scala, Java, Python and R.
In Scala+Java, DF is represented as a Dataset of Rows.
	Scala: Dataframe is simple a type alias of Dataset[Row]
	Java: Dataset<Row>
Optimization Engine is known as the Catalyst Optimizer.
Can find the schema automatically.
Faster than RDDs and DS.

RDDs:
There is no built-in optimization engine for RDDs.
Schema needs to be defined manually.
Are slower than both DS and DF.

Temp View vs Global Temp View created from a DF:
To create:
df.createOrReplaceTempView("name of the temp view")			===> Temp View
df.createGlobalTempView("name of the global temp view")		===> Global Temp View

To use:
spark.sql("SELECT * FROM tempview")
spark.sql("SELECT * FROM global_temp.globaltempview")

TempView: Is available as long as the Spark Session is active, but within the same Spark Session only.
GlobalTempView: Is available across Spark Sessions.
    spark.newSession().sql("SELECT * FROM global_temp.people").show()		// Works.
    spark.newSession().sql("SELECT * FROM people").show()     				// Does not work.

Read a json file, create a DF and convert it into a DS of type Person:
case class Person(name: String, age: Long)
Create a DF:
val peopleDF = spark.read.json(path)
Convert the DF into a DS:
val peopleDS = peopleDF.as[Person]

val peopleDS = spark.read.json(path).as[Person]

Read text file into an RDD and convert it into a DF:
- returns an RDD: 
	spark.sparkContext.textFile("file:///home/maria_dev/SparkSamples/resources/spark_examples/people.txt")
	similar to sc.textFile("filename")
- split the lines into words separate by comma:
	.map(_.split(","))
- map the attributes from the RDD (file read) to the Person class and create objects of type Person for each row:
	.map(attributes => Person(attributes(0), attributes(1).trim.toInt))
		- read the attributes in each line.
		- map the name (attribute(0)) to the Name property of the Person class.
		- map the age (attribute(1)) to the Age property of the Person class.
			- but since the text file has a blank space in the age value, we have to trim the spaces first before converting it into an integer.
- Convert the resulting RDD into a DF:
	.toDF()

spark.sparkContext.textFile: 	returns a RDD.
spark.read.json:				returns a DF.
	Here, "spark" is the SparkSession object.


Creating a schema programmatically:
You need an object of type StructType.
This StructType will have objects of type StructField for each field in the schema.

For e.g.:
	- Id
	- Firstname
	- Lastname
	- City
	- Age

Create the schema of type StructType:
StructType(
	Array(
		StructField("Id", StringType, nullable=true),
		StructField("Firstname", StringType, nullable=true),
		StructField("Lastname", StringType, nullable=true),
		StructField("City", StringType, nullable=true),
		StructField("Age", IntType, nullable=true)
	)
)

Struct means Structure.

Example code:
    // Specify the schema required in a string.
    val schemaString = "name age"

    // Generate the schema based on the string of schema
	// Split the string schema into words separated by " ".
	// For each word (which is the column/field name), convert it into an object of type StructType
	// Return the result into a collection of StructField objects.
    val fields = schemaString.split(" ")
      .map(fieldName => StructField(fieldName, StringType, nullable = true))

	// The above code will return this:
	//StructField("name", StringType, nullable = true)
	//StructField("age", StringType, nullable = true)
	  
	// Create the StructType object that has the collection of fields of type StructField:
    val schema = StructType(fields)
	// This will give:
	//StructType (
	//	StructField("name", StringType, nullable = true),
	//	StructField("age", StringType, nullable = true)
	//)
	
	// To create a DF from a RDD, use the spark.createDataFrame() method.
	// Create a DF by applying the schema to a RDD.
	val df = spark.createDataFrame(rdd, schema)			
	
	
This will also work:
	val df = spark.createDataFrame(rdd)


Different ways to create DataFrames:
------------------------------------
1. Create a DF from a RDD by using .toDF() on the RDD.
For e.g.; val df = rdd.toDF()

2. Create a DF from a RDD using .toDF() and specifying column names:
For e.g.; val df = rdd.toDF("name", "age")

3. Create a DF using SparkSession object's createDataFrame() method:
For e.g.; val df = spark.createDataframe(rdd).toDF()

4. Create a DF using SparkSession object's createDataFrame() method and also specify the schema as a collection:
For e.g.: 
val columns = Seq("language", "users_count")
val df = spark.createDataframe(rdd).toDF(columns:_*)

5. Create a DF using SparkSession object's createDataFrame() with Row type:
    val schema = StructType(
      Array(
        StructField("language", StringType, true),
        StructField("users_count", StringType, true)
      )
    )
	
	Here, we are creating a StructType object that has a collection (Array) of StructField objects.
	Each StructField object is for a "column" in the schema.
	
	Create an RDD of Row type for each row in the original RDD:
	val rowRDD = rdd.map(attributes => Row(attributes._1, attributes._2))
	
	Create the DF by applying the schema defined (the StructType object):
	val dfFromRDD3 = spark.createDataFrame(rowRDD, schema)

6. Create a DF from a Scala collection (Seq, List etc.) using .toDF():
val data = Seq(("Java", "20000"), ("Python", "100000"), ("Scala", "3000"))
val df = data.toDF()

The above example will not have a "proper" schema for the DF and hence will print _1 and _2 for the column names.

7. Create a DF from a Scala collection (Seq, List etc.) and specify a Schema using spark.createDataFrame().toDF():
val columns = Seq("language", "users_count")
val data = Seq(("Java", "20000"), ("Python", "100000"), ("Scala", "3000"))		// A Sequence of Tuples.
var dfFromData2 = spark.createDataFrame(data).toDF(columns: _*)

8. Create a DF from a Seq (or List) of Row types using spark.createDataFrame() and specify schema as well:
val columns = Seq("language", "users_count")
val rowData = Seq(Row("Java", "20000"), Row("Python", "100000"), Row("Scala", "3000"))		// Sequence of Rows.
var dfFromData3 = spark.createDataFrame(rowData, schema)

9. Create a DF from a csv file using spark.read.csv():
    val df2 = spark.read.csv(
      "file:///home/maria_dev/SparkSamples/resources/spark_examples/people.csv"
    )
    df2.printSchema()
    df2.show()
	
10. Create a DF from a json file using spark.read.json().
11. Create an empty DF using spark.emptyDataFrame
12. Create an empty DF using spark.createDataFrame with a schema:
    val schemaEmptyDF = StructType(
		StructField("firstName", StringType, true) ::
        StructField("lastName", IntegerType, false) ::
        StructField("middleName", IntegerType, false) :: Nil
    )

    val dfEmpty2 =
      spark.createDataFrame(spark.sparkContext.emptyRDD[Row], schemaEmptyDF)
    dfEmpty2.printSchema()

13. Create an empty DF from a sequence.
    val colSeq = Seq("firstName", "lastName", "middleName")
    val dfEmpty3 = Seq.empty[(String, String, String)].toDF(colSeq: _*)

14. Create an empty DF using a case class:
case class Name(firstName: String, lastName: String, middleName: String)
    val dfEmpty4 = Seq.empty[Name].toDF()       // case class defined outside of "main".
    dfEmpty4.printSchema()

15. Create a DataSet from a RDD using SparkSession's createDataset() method:
    val ds = spark.createDataset(rdd3)
	
	
.show():
--------
By default, show() will display only the first 20 records in the DF.
Overloaded methods of show():
show(): Display first 20 records. Values are truncated to first 20 characters.
show(numRows: Int):	Display first "numRows" records.
show(truncate: Boolean): Display full content of values and not truncated. Default is true.
show(numRows: Int, truncate: Boolean): Display first "numRows" records and do not truncate column values.
show(numRows: Int, truncate: Int): Display first "numRows" records and display first "truncate" chars of values.
show(numRows: Int, truncate: Int, vertical: Boolean): Display first "numRows" records and display first "truncate" chars of values and display the rows vertically (if true. Default is false).

Different ways to define schemas using StructType:
1. Simple schema using Array:
    val simpleSchema = StructType(
      Array(
        StructField("firstname", StringType, true),
        StructField("middlename", StringType, true),
        StructField("lastname", StringType, true),
        StructField("id", StringType, true),
        StructField("gender", StringType, true),
        StructField("salary", IntegerType, true)
      )
    )
	
2. Nested schema using .add:
    val structureData = Seq(
      Row(Row("James ", "", "Smith"), "36636", "M", 3100),
      Row(Row("Michael ", "Rose", ""), "40288", "M", 4300),
      Row(Row("Robert ", "", "Williams"), "42114", "M", 1400),
      Row(Row("Maria ", "Anne", "Jones"), "39192", "F", 5500),
      Row(Row("Jen", "Mary", "Brown"), "", "F", -1)
    )
	
    val structureSchema = new StructType()
      .add(
        "name",
        new StructType()
          .add("firstname", StringType)
          .add("middlename", StringType)
          .add("lastname", StringType)
      )
      .add("id", StringType)
      .add("gender", StringType)
      .add("salary", IntegerType)
	  
	The above can be split as follows:
    val structureSchema = new StructType()
	structureSchema.add("name",
        new StructType()
          .add("firstname", StringType)
          .add("middlename", StringType)
          .add("lastname", StringType)
		)
	structureSchema.add("id", StringType)
	structureSchema.add("gender", StringType)
	structureSchema.add("salary", IntegerType)

	OR:
	val fullname = new StructType()
	fullname.add("firstname", StringType)
	fullname.add("middlename", StringType)
	fullname.add("lastname", StringType)

	val structureSchema = new StructType()
	structureSchema.add("name", fullname)
	
	structureSchema.add("id", StringType)
	structureSchema.add("gender", StringType)
	structureSchema.add("salary", IntegerType)

Select Columns from a DF:
-------------------------
select() is used to select column(s) from a DF.
You can select
	- one or more columns,
	- nested columns,
	- cols by index,
	- all columns,
	- from a list
	
1. Specify column names as strings:
df.select("colname1", "colname2"...).show()
2. Specify col names as strings on the DF object:
df.select(df("colname1"), df("colname2")...).show()
3. Specify col names with the col(). Provide an alias, if required.
df.select(col("colname1").alias("aliasname"), col("colname2")...).show()
4. Select all cols:
    df.select("*").show()
5. Extract all cols into a collection and then give that collection to select():
    val columnsAll = df.columns.map(m => col(m))
    df.select(columnsAll: _*).show()
6. The same thing, but in a single operation.
    df.select(columns.map(m => col(m)): _*).show()
7. Specify cols from a list (very similar to 5 & 6 above):
    val listCols = List("lastname", "country")
    df.select(listCols.map(m => col(m)): _*).show()
8.  Select First N Columns.
    Select Column By Position or Index//Select first 3 columns:
    df.select(df.columns.slice(0, 3).map(m => col(m)): _*).show()
9.  Select Column By Position or Index//Select first 3 columns.
    Selects 4th column (index starts from zero):
    df.select(df.columns(3)).show()
    Selects columns from index 2 to 4:
    println("select columns 2 to 4 using slice() (upto 4, not including 4)...")
    df.select(df.columns.slice(2, 4).map(m => col(m)): _*).show()
10. Nested cols:
    df2.select("name").show(false)		// displays the nested col as a single value.
    // to get the specific column from a struct, you need to explicitly qualify.
    df2.select("name.firstname","name.lastname").show(false)
    // to get all columns from struct column.
    df2.select("name.*").show(false)	// displays each sub-column as an individual value.
	
collect():
----------
It is an operation (action) used to retrieve the data from a DF.
Retrieve all the elements of the RDD/DF/DS.
Use collect() after you perform a filter(), group(), count().
Caution: do not perform collect() on a very large dataset! If you do, you may run out of memory!
collect() vs select().
	- select() returns a RDD/DF that holds the columns that are selected.
	- collect() returs the entire contents of the RDD/DF/DS.
	- select() is a Transformation.
	- collect() is an Action.

withColumn():
-------------
It is a DF function used to:
	- add a new column to the DF.
	- change the value of an existing column.
	- convert the data type of a column.
	- derive a new column from an existing column.
	
withColumnRenamed():
--------------------
	Used to rename a column.
	val dfRen=df2.withColumnRenamed("gender","sex")

drop():
-------
Used to drop one or more columns from a DF/DS.
-df.drop("colname")
-df.drop(df("colname"))
-df.drop(col("colname"))
-df.drop("firstname","middlename","lastname")	// Drop multiple cols.
-val cols = Seq("firstname","middlename","lastname")
 df.drop(cols:_*)								// Drop multiple cols from a collection.

filter() and where():
---------------------
Both work exactly the same way.
If you are from SQL background, you may prefer to use where().

df.filter(df("state") === "OH").show(false)
df.filter("gender == 'M'").show(false)
df.filter(df("state") === "OH" && df("gender") === "M").show(false)
df.filter(array_contains(df("languages"),"Java")).show(false)
df.filter(df("name.lastname") === "Williams").show(false)

sort() or orderBy():
--------------------
Used to sort by either ascending or descending order, based on single column or multiple columns.
using sort():
	df.sort("department","state").show(false)
using col() with sort():
	df.sort(col("department"),col("state")).show(false)
using orderBy():	
  df.orderBy("department","state").show(false)
using col() with orcerBy():
  df.orderBy(col("department"),col("state")).show(false)
In ASC order:
	df.sort(col("department").asc,col("state").asc).show(false)
	df.orderBy(col("department").asc,col("state").asc).show(false)
In DESC order:
	df.sort(col("department").asc,col("state").desc).show(false)
	df.orderBy(col("department").asc,col("state").desc).show(false)
Using SQL Syntax:
	df.createOrReplaceTempView("EMP")
	spark.sql(" select employee_name,department, state,salary,age,bonus from EMP ORDER BY department ASC, state DESC").show(false)

groupBy():
Similar the SQL "GROUP BY" clause.
Used to collect data into groups from the DF/DS and perform aggregation on the grouped data.
Aggregate functions:
count()
mean()
max()
min()
sum()
avg()
agg()

join():
-------
Very similar to SQL joins and there are:
INNER
LEFT OUTER
RIGHT OUTER

join() results in Wider Transformations (data is shuffled across partitions).

- inner:
	empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"inner")
	SELECT ... FROM emp INNER JOIN dept ON emp.dept_id = dept.dept_id

- outer, full, fullouter:
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"outer")
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"full")
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"fullouter")

- right, rightouter:
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"right")
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"rightouter")

- left, leftouter:
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"left")
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"leftouter")

- leftsemi:
  // Similar to INNER JOIN, returns all columns from the left DF/DS for matching records
  // and ignores all columns from the right DF/DS.
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"leftsemi")

- leftanti:
  // Returns only columns from the left DF/DS for non-matched records.
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"leftanti")

- crossjoin as part of join():
	Similar to inner join, except that you are specifying the type as "cross" but with a match condition.
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"cross")

- crossJoin():
  empDF.crossJoin(deptDF).show(false)
	SELECT * FROM emp, dept
	
- self join:
  empDF.as("emp1").join(empDF.as("emp2"),
    col("emp1.superior_emp_id") === col("emp2.emp_id"),"inner")
    .select(col("emp1.emp_id"),col("emp1.name"),
      col("emp2.emp_id").as("superior_emp_id"),
      col("emp2.name").as("superior_emp_name"))
	  
distinct() dropDuplicates():
----------------------------
Remove / drop duplicate rows.
distinct(): used to remove rows that have the same values on all columns in the DF.
dropDuplicates(): used to remove rows that have the same values on multiple selected columns.

val distinctDF = df.distinct()
val df2 = df.dropDuplicates()	// same as df.distinct()

val dropDisDF = df.dropDuplicates("department","salary")

map() and flatMap():
--------------------
map()	 : Spark map() function applies a function to each in the DF/DS and returns a new transformed DF/DS.
flatMap(): Spark flatMap() function transformation flattens the DF/DS after applying the function on every element and returns a new transformted DS/DF.
	flatMap() returns more rows than in the original DF/DS.
	a.k.a. One-to-many transformation function.
	
	Spark flatMap() will take each element in a list and convert it into rows of those elements.
	So, [Java, Python, C++, VB] will be converted into rows as:
		Java
		Python
		C++
		VB

Points to note:	
- Both map() and flatMap() return a DS.
- Both these transformations are narrow, meaning they do not result in shuffles.
- flatMap() results in redundant data on some columns.
- map() always returns the same size/records as the input DF., whereas flatMap() returns many records for each record in the input DF (one-to-many).

union() and unionAll():
-----------------------
union():
- Used to combine two DFs that have the same schema/structure.
- If the schema/structure is not the same, it returns an error.
- Originally, it used to eliminate duplicate records.
- But after Spark 2.0.0, they removed unionAll() and replaced it with union() and hence, it shows duplicate records as well.

unionAll(): deprecated since Spark 2.0.0.
- Combines the two DS/DF including the duplicate records.
- Replaced with union().

Working with JSON Datasets.


Working with Parquet files:
===========================
Apache Parquet is a columnar file format that provides optimization to speed up queries.
It provides far more efficient file format than CSV, JSON and XML.
It is supported by many data processing systems, including Hadoop, HDFS, Spark, Scala, Python.
Can also be used with SparkSQL.

Reduces storage by almost 75% on average.
It also captures the schema of the data automatically when you read and write Parquet files.

Advantages:
- Reduces IO operations.
- Fetches specific columns that you need to access.
- Consumes less space.

The .toDF() function on a Sequence collection is only available when you import the spark.sqlContext.implicits._ package.

Unstructured social media data processing using Spark:
======================================================
Sales Details
20220425

Sales Person: John

Code Product Quantity Price
A0001 Product1 200 79.50
A0002 Product2 145 198.99

SparkSQL Shuffle Partitions:
============================
The Spark SQL Shuffle is a mechanism for redistributing / repartitioning the data so that the data is grouped differently across partitions, based on the size of the data you may want to either reduce or increase the no. of partitions of your RDD/DF.
Do this with the help of spark.sql.shuffle.partitions configuration option or from code.

Spark Shuffle is a very expensive operation as it moves data between executors or even between worker nodes in the cluster.
Use it wisely.
Reasons:
- Disk I/O.
- Involves data serialization and deserialization.
- Network I/O.

Some transformations operations that result in shuffling:
groupByKey()
reduceByKey()
sortByKey()
countByKey()
join()
union()
repartition()

getNumPartitions()
repartition(n)

Spark Default Shuffle Partitions:
DF increases the number to 200 automatically when Spark operation performs data shuffling (join(), union(), aggregate functions etc.).
this default partition number is defined by the property spark.sql.shuffle.partitions in the SparkSQL configuration.

It can be changed as follow in your code:
spark.conf.set("spark.sql.shuffle.partitions", 50)

