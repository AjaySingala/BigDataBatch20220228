Apache Spark:
No. 1 Big Data tool these days to process datasets and perform analysis.
It is even considered to be a "Hadoop Killer".
Spark has advantages over Hadoop.
Spark is written in Scala.

Hadoop, which uses MapReduce (MR) for processing, performs of lot Disk I/O.
Spark completely avoids Disk I/O. It does processing in-memory! 
In-memory cluster computing is the main feature of Spark.
Because of this, Spark is considered ~100 times faster than Hadoop MR.

Spark distributes processing across nodes, which is why it fault-tolerant and highly scalable.
With Spark, you can use AWS S3 instead of HDFS for storage.

Spark is NOT a modified version of Hadoop.

Features:
Speed: due to in-memory cluster computing.
Supports multiple languages: Scala, Java, Python. There are some features of Spark that do not work with Python.
Advanced Analytics: Has it's own "map" & "reduce". Also supports SQL queries, Streaming data, Machine Learning (ML) and Graph algorithms.

Hadoop vs Spark:
----------------
					Hadoop MapReduce			Spark
					------------------------------------------------------
Storage				Disk only.					In-memory or disk.
Operations			Map and Reduce.				Map, Reduce, Join, Group etc.
Execution Model		Batch						Batch, streaming, interactive.
Programming env.	Java, Python Scala.			Java, Scala and Python (with limitations in Python). Also supports R.

Spark Architecture:
Driver program listens for incoming connections, accepts them and gives them to the Worker Node for execution.
Driver program co-ordinates with the Executor on each node.
Each Worker Node has 1 Executor, some Cache storage and tasks that are executed (and monitored by the Executor).
You can configure the amount of memory that will used by the executor. You can do this by setting a property called spark.executor.memory.

How to determine no. of executors for a job:
Nodes = 10.
Each node has 16 cores (-1 for OS).
Each node has 61GB RAM (-1 for OS).
A general rule of thumb is an executor can run 5 concurrent tasks in parallel.
No. of cores available is 15.
No. of executors = no. of cores available / concurrent tasks = 15 / 5 = 3.

= no. of nodes * no. of executors on each node
= 10 * 3
= 30 executors per Spark Job.

RDD - Resilient Distributed Datasets:
--------------------------------------
Fundamental data structure in Spark.
It is an immutable distributed collection of objects.
Each dataset in RDD is divided into logical partitions, which may be computed on different nodes of the cluster.
RDDs can contain objects of Python, Java, Scala. Even objects of classes you create in your code.
Resilient because they are able to withstand failure / recover quickly from failure. They are fault-tolerant.

There are 2 ways in which you can create an RDD:
1. parallelizing existing collections
2. Referencing a dataset read from an external storage system like HDFS, AWS S3, HBase etc.

Data Sharing in MapReduce is slow.
	because MR does a lot of Disk i/o.
	
The RDDs with the data are processed in-memory by Spark when you run queries etc. on the RDDs.
Whereas MapReduce would have used disk to share data between iterations.

Two ways Data Sharing works with Spark RDD:
1. Iterative Operations on Spark RDD.
	Store intermediate results in distributed memory which is used by different iterations.
	
2. Intreactive Operations on Spark RDD.
	Different queries are executed on the same set of data repeatedly.
	
It may happen that when data is stored in distributed memory, the node(s) run out of memory. Then in such cases, the intermediate results are stored on disk.

Spark Core Programming:
Open the spark-shell
read a file from the local disk on the VM:
val file = sc.textFile("file:///home/maria_dev/SparkSamples/sparkscala/src/main/scala/example/twinkle.txt")

if we are reading from HDFS:
val file = sc.textFile("twinkle.txt")		// Path would be /user/maria_dev/twinkle.txt

We have to count no. of words in the file.
val wordCount = file.flatMap(line => line.split(" ")).map(word => (word,1)).reduceByKey(_+_)

Steps are:
read each line
split into words separated by the " "
initiaze each word with 1. For e.g.; (Twinkle, 1) where the word is the "key" and the number, 1 in this case, is the value.
finally, reduce the keys by adding values of similar keys.

For e.g.;
twinkle twinkle little star
(twinkle,1)
(twinkle,1)
(litte,1)
(star,1)

Save the resulting RDD to HDFS:
wordCount.saveAsTextFile("twinkle-output")		// path is /user/maria_dev/twinkle-output

It creates a folder on HDFS with the given name and that folder will have 1 or more files with result as follows:
(diamond,1)
(are,1)
(high,1)
(how,1)
(twinkle,4)
(star,2)
(what,1)
(so,1)
(little,2)
(world,1)
(wonder,1)
(up,1)
(you,1)
(a,1)
(above,1)
(I,1)
(in,1)
(like,1)
(the,2)
(sky,1)

Save the resulting RDD to local disk on VM:
wordCount.saveAsTextFile("file:///home/maria_dev/tmp/twinkle-output")		// path is /user/maria_dev/twinkle-output

This will create a folder on your local disk on VM at /home/maria_dev/tmp/twinkle-output, which will have 1 or more files with the result as mentioned above.

We did 2 things here:
1) Transformation.
2) Action.

RDD Transformations:
They return a new RDD and also allows you to create dependencies between RDDs, thus creating a dependency chain by creating one RDD from another.
This is known as RDD Lineage.

Some transformation functions are:
flatMap()
map()
reduceByKey()

RDD Transformations in Spark are "LAZY"!!!!
Meaning, in the following code:
val file = sc.textFile("twinkle.txt")
val wordCount = file.flatMap(line => line.split(" ")).map(word => (word,1)).reduceByKey(_+_)
wordCount.saveAsTextFile("twinkle-output")

The file is not processed (the flatMap, Map and reduceByKey operations are not performed) when that particular is executed!
It is executed / processed ONLY when you perform an "ACTION" on the RDD!!!
So, when you exeucted .saveAsTextFile(), that is when the RDD was processed, that is when the file was actually read.

To determine no. of partitions of an RDD use getNumPartitions:
println(s"Initial Partition count: ${file.getNumPartitions}")					// prints 2.

To change the no., of partitions, use repartition(n):
val repartitionedRDD = file.repartition(4)			// Transformation.
println(s"Initial Partition count: ${repartitionedRDD.getNumPartitions}")		// prints 4.

Some Transformations and Actions:
.collect()	: Action: Returns/Collects the data in the RDD.
	file.collect()
	wordCount.collect()
.foreach()	: Action: iterate the data and process.
	file.collect().foreach(println)
	wordCount.collect().foreach(println)
.flatMap: Transformation.
	val rdd2 = file.flatMap(f => f.split(" "))
	rdd2.foreach(f => println(f))
.map: Transformation.
	val rdd3 = rdd2.map(m => (m,1))
	rdd3.foreach(println)
.filter(): Transformation.
	val rdd4 = rdd3.filter(f => f._1.startsWith("a"))
	rdd4.foreach(println)
.reduceByKey: Transformation.
	val rdd5 = rdd3.reduceByKey(_ + _)
	rdd5.foreach(println)
.sortByKey: Transformation
	// Swap the K-V and then sort.
	val rdd6 = rdd5.map(w => (w._2, w._1)).sortByKey()
	rdd6.foreach(println)
.count:	Action.
	println(s"There are ${rdd6.count()} words...")
.first: Action.
	val firstRecRDD = rdd6.first()
	println(s"First record: ${firstRecRDD._1} : ${firstRecRDD._2}")
.max: Action
	val maxRDD = rdd6.max()
	println(s"Max record: ${maxRDD._1} : ${maxRDD._2}")
.reduce: Action.
	val totalWordCountRDD = rdd6.reduce((a,b) => (a._1 + b._1, a._2))
	println(s"${totalWordCountRDD._1}")
.take: Action.
	val takeRDD = rdd6.take(3)
	takeRDD.foreach(println)
	takeRDD.foreach(f => { println(s"Key: ${f._1}, Value: ${f._2}") } )
.collect: Action.	
	val collectRDD = rdd6.collect()
	collectRDD.foreach(f => { println(s"Key: ${f._1}, Value: ${f._2}") } )
.saveAsTextFile: Action.
	rdd6.saveAsTextFile("wordCount")					// Creates a folder on HDFS.
	rdd6.saveAsTextFile("file:///tmp/wordCount")		// Creates a folder on local machine (VM).
	
Creating RDDS using parallelizing technique:
--------------------------------------------
val parallelRDD = sc.parallelize(Seq(("A", 1), ("B", 2), ("C", 3)))
parallelRDD.collect().foreach(println)
val parallelRDD2 = sc.parallelize(Seq(("B", 4), ("E", 5)))
parallelRDD2.collect().foreach(println)

val cogroupRDD = parallelRDD.cogroup(parallelRDD2)
cogroupRDD.collect().foreach(println)

Spark Basics Overview:
----------------------
RDD - Resilient Distributed Datasets
	Resilient - means able to withstand or recover quickly from failure / difficult conditions.
	Fault-tolerant
	val rdd1 = sc.textFile("....")
	val rdd2 = rdd1.flatMap(...)
	val rdd3 = rdd2.map(...)
	val rdd4 = rdd3.reduceByKey(...)
	
Transformations return a RDD.
RDDs are immutable.
Transformations are lazily evaluated.
	They will be availuated when you perform an Action on the RDD.
	
Some Actions:
rdd1.collect()
rdd2.count()
rdd3.collect().foreach()
rdd4.saveAsTextFile("...")

RDD Lineage: when you create a RDD from another RDD. Basically, create a "lineage".
	For e.g.; created rdd4 from rdd3. rdd3 from rdd2 and rdd2 from rdd1.
	
Spark Deploy Modes:
-------------------
Two Deploy Modes:
1. Local (client).
2. Cluster Mode.

1. Client Mode (Local):
When the driver program runs on the same machine from which the job was submitted.
For e.g.; a single node cluster or you do a remote connection to a server on the cluster and run the Spark job manually either using spark-shell or spark-submit.
The problem with this mode is that if the driver program fails, then the entire job fails.
Not very efficient from a performance perspective (because you are running it locally). You are not making use of the cluster.
This mode will never be used in a production (live) environment.

To run a jonb in client mode:
spark-submit --deploy-mode client ....
For e.g.; spark-submit ./scalahive_2.11-0.1.0-SNAPSHOT.jar  --class example.HiveDemo --deploy-mode client

This mode (client mode) is also the default mode when you start the spark-shell.

2. Cluster mode:
The driver program does not run on the machine from where the job was submitted. It runs on the cluster as a sub-process of the Application Master.
Makes use of the cluster features.
If the driver program crashes or fails, it will automatically be re-instantiated by YARN.

To run a job in cluster mode:
spark-submit --deploy-mode cluster ....
For e.g.; spark-submit ./scalahive_2.11-0.1.0-SNAPSHOT.jar  --class example.HiveDemo --deploy-mode cluster

Here, the application / spark jon will run on the cluster.
Cluster mode is not supported in the interactive shell mode (spark-shell).

Cluster mode is used in a production (live) environment.

Steps to run scala files from spark-shell:
1. Start spark-shell (from the folder where the .scala files are located).
2. :load <scala filename>
Note: In your scala code, comment the lines where you:
	- create the SparkSession and SparkContext objects.
	- specify the pakcage name (at the top).
	- create the object and the main method.
	- and the closing curly-braces (}) for the object and main method (at the end of the code).

Steps to run scala code using spark-submit:
1. Import packages:
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession

2. Create SparkSession:
    val spark:SparkSession = SparkSession.builder()
      .master("local[3]")
      .appName("ajaysingala.com")
      .getOrCreate()
	  
	 here, "local[3]" means run the code locally and use 3 cores. To use all cores, specify "[*]".
	 
3. Create the SparkContext:
    val sc = spark.sparkContext

1. compile your sbt project.
2. create the jar file (run package command in sbt).
3. copy the jar file to your vm using the scp command:
scp -P 2222 .\sparkscala_2.11-0.1.0-SNAPSHOT.jar maria_dev@sandbox-hdp.hortonworks.com:/home/maria_dev

4. Run the job as follows:
spark-submit --deploy-mode client ./sparkscala_2.11-0.1.0-SNAPSHOT.jar  --class sparkbasics.WordCountExample 


SparkSession and SparkContext:
------------------------------
SparkContext (SC):
In earlier versions of Spark (or PySpark), SC was the entry point to Spark programming with RDD and connect to cluster.
With Spark 2.0 SparkSession (SS) has been introduced as the entry point from which you create the SC.
Also, with Spark 2.0, SS became the entry point for programming with DataFrame and DataSet.

SC is used to programmatically create RDD, accumulators and broadcast variables on the cluster.
With Spark 2.0, most of the functionalities in SC are available in SS as well.

In spark-shell, the SparkContext is available by default in the variable named "sc".

SparkSession:
Introduced in Spark 2.0.
Is an entry point to the underlying Spark functionality to programmatically create Spark RDD, DataFrame and DataSet.

In spark-shell, the SparkSession is available by default in the variable named "spark".

SS also includes APIs for different contexts:
- Spark Context
- SQL Context
- Hive Context
- Streaming Context

- Create SparkSession:
    val spark:SparkSession = SparkSession.builder()
      .master("local[3]")
      .appName("ajaysingala.com")
      .getOrCreate()
	  
	 here, "local[3]" means run the code locally and use 3 cores. To use all cores, specify "[*]".
	 
- Create the SparkContext:
    val sc = spark.sparkContext

RDD Lineage a.k.a. RDD Dependency Graph.
Transformations are of 2 types:
- Narrow Transformations.
map() and filter()
All the elements that are required to compute the records in single partition live in the single partition of the parent RDD.
a limited subset of partition is used to calculate the result.
These transformations compute the data that live on a single partition. There will not be any movement of data between partitions to execute narrow transformations.

- Wide Transformations.
All the elements that are required to compute the records in the single partition may live in many partitions of the parent RDD.
These transformations compute data that live on may partitions. There will be data movement between partitions to execute wide transformations.
Since this data movement "shuffles" data between partitions, it is known as shuffle transformations.
reduceByKey(), groupByKey(), repartition()

Shared Variables:
=================
2 types of Shared Variables:
- Broadcast Variables: used to efficiently distribute large datasets across nodes.
- Accumulators: used to aggregate information of a collection.

Broadcast Variables (BVs):
They are cached on each machine (node) rather than making a copy of the data with the tasks.
Spark uses very efficient broadcast algorithms to distribute broadcast variables so as to reduce cross-node communication. Hence it is much faster.
By using BVs, you reduce the number of shuffles and hence increase the performance.

How to create and use BVs:
1. Define a collection:
  val states = Map(("NY","New York"),("CA","California"),("FL","Florida"), ("NSW", "New South Wales"))
  val countries = Map(("USA","United States of America"),("AU","Australia"))

2. Create a BV using spark.sparkContext.broadcast():
  val broadcastStates = spark.sparkContext.broadcast(states)
  val broadcastCountries = spark.sparkContext.broadcast(countries)
  
  OR
  
  val broadcastStates = sc.broadcast(states)
  val broadcastCountries = sc.broadcast(countries)
 
3. Create the source data:
  val data = Seq(("James","Smith","USA","CA"),
    ("Michael","Rose","USA","NY"),
    ("Robert","Williams","USA","CA"),
    ("David","Warner","AU","NSW"),
    ("Maria","Jones","USA","FL")
  )

4. Parallelize the source data:
  val rdd = spark.sparkContext.parallelize(data)  // sc.parallelize(data)

5. Process the data and extract names of states and countries from BVs:
  val rdd2 = rdd.map(f=>{
    val country = f._3
    val state = f._4
    val fullCountry = broadcastCountries.value.get(country).get
    val fullState = broadcastStates.value.get(state).get
    (f._1,f._2,fullCountry,fullState)
  })

6. Print the original source data:
  rdd.foreach(println)

7. Print the data with values extracted from BVs:
  println(rdd2.collect().mkString("\n"))

Accumulators:
Are shared variables used for aggregating data across executors (nodes).
Similar to how MapReduce counters.

Log file: City,State,Product,Amount
Sample data:
-------------------------
Dallas,TX,Erasers,100

Orlando,FL,Pencils,125
Bad data packet
Boston,MA,Markers,0
--------------------------

In the log file, there are some records that are corrupted.
For e.g.; 2nd line is blank. 4th line has some error info. Last line has sales amount as 0 (not possible).

Determine how many blank lines are there in the log file.

var blankLinesCount: Int = 0
sc.textFile("sales.log")
	.foreach { line => if(line.length == 0) blankLinesCount += 1 }

println(s"There are $blankLinesCount blank lines in the file.")

This will print "There are 0 blank lines in the file."
This is because when Spark distributes (ships) this code to every executor, the variable "blankLinesCount" becomes local to that executor.
So, it's updated value is not relayed back to the driver program.
To avoid this, we can make "blankLinesCount" variable as an "Accumulator" such that all the updates from all executors to this variable is relayed back to the driver.

var blankLinesCount: sc.accumulator(0, "Blank Lines")
sc.textFile("sales.log")
	.foreach { line => if(line.length == 0) blankLinesCount += 1 }

println(s"There are $blankLinesCount blank lines in the file.")

Reading data files into RDD:
============================
Read files using the sc.textFile method.
It takes 2 params:
	- 1st is the path to the file(s).
	- 2nd is an optional param to specify no. of partitions.
	
val rddFromFile = spark.sparkContext.textFile("file:///home/maria_dev/SparkSamples/resources/csv/text01.txt")
val rddFromFile = spark.sparkContext.textFile("file:///home/maria_dev/SparkSamples/resources/csv/text01.txt", 3)

Wildcards:
* means 0 or more characters
Text*.txt: Text.txt Text0.txt Text01.txt Text001.text Texts00000001.txt
? means 1 and only 1 character.
Text?.txt: Text0.txt Text1.txt TextA.txt Text_.txt
Text??.txt: Text00.txt Text01.txt Text0A.txt Text_A.txt

Paired RDDs:
============
Key-Value Pair RDDs.
RDDs that have key-value pairs in them.


Transformations:
	-aggregate()
	-aggregateByKey()

These transformations take 3 parameters:
Param1: is the initial value to begin aggregation (which usually is 0).
Param2: (Sequence Operator) the operator (function to perform) to accumulate the results of each partition, and stores the running accumulated result and returns it to the driver.
Param3: (Combined Operator) This operator is used to combine the results of all partitions and give the final result.

AWS EMR:
========
AWS = Amazon Web Service. Cloud platform from Amazon. Cloud Service Provider (CSP).
Other CSPs are Microsoft Azure, Google Cloud Platform (GCP) and many more.
EMR = Elastic MapReduce.
	- The Hadoop Cluster you can setup on AWS.
	- You don't have to install anything. The Hadoop cluster is readily available.
	- You just select you want on the cluster and provision it.
	- AWS will then provision some VMs (EC2 (Elastic Compute Cloud) instances) and have the Hadoop cluster configured on them.
	
Big Data on Cloud:
- on AWS: AWS EMR.
- on Azure: MS Azure HDInsight.
- on GCP: Google DataProc.

Storage is a big decision when hosting Hadoop cluster on cloud.
Options are:
- HDFS
- AWS S3

What is AWS S3:
---------------
S3 is a distributed managed file system provided by AWS.
Similar to HDFS: distributed, fault-tolerant etc.

Why S3 and not HDFS?
When you provision a Hadoop Cluster on AWS EMR and use HDFS for storage, and run a spark job on the cluster.
The job will read an input file from HDFS, process it, write the results back to HDFS.
Remember:
	- on AWS (or for that matter any CSP), you will be charged fo the time the VMs are running. so as long as the VMs (instances) are running, you will be charged of it.
	- After the Spark job has completed and the instances are still running (doing nothing, just idle), you still get charged for it because the instances are running.
Ideally, after the Spark job completes, the cluster is "de-provisioned" or "terminated". This is not same as "Start" and "Stop".
This is so that you do not get charged for idle instances.
But, when the cluster terminates, the instances are not accessible and hence the HDFS storage is also not accessible.
Which means the result generated and saved by the Spark job is lost!!!
What's the solution to this?
	Keep the cluster running and keep paying for idle time!
	Not advisable!
Alternate solution:
	Use AWS S3!
	It does not depend on any VM instance to be idle/ terminated / running etc.
	You still run your Spark job on the AWS EMR (Hadoop) Cluster.
	But use S3 as the storage instead of HDFS.
	Your input file will be on S3.
	And the Spark job will save/store the results on S3 as well.
	So, when the job complets and the cluster is terminated, you still have the data (input and the output) on S3 which is accessible.
	In this way, you do not pay for idle instances.
	S3 is decoupled from the AWS EMR cluster.
	You only pay for the storage used and not the VMs (EC2 instances of the cluster).
	
How to use AWS Cloud:
1. You have to register.
2. There are some services that are free.
3. But there are many that are not free. You have to pay for those. AWS EMR is one of them.
4. You need a credit card to register on AWS.
5. Billing is per month.

AWS S3: IaaS, SaaS
S3 is a distributed managed file system provided by AWS.
It is very similar to any file system where you can organize your files in folders and sub-folders.
On S3, you have to first create a "bucket" and within a bucket, you can then create as many folders and sub-folders as you want.

EC2: Elastic Compute Cloud: IaaS
===========================
When you provision a VM on AWS, you are actually using the EC2 service of AWS.
The VM is known as an "EC2 instance".
So, when we provision an AWS EMR cluster, it will be made of some EC2 instances.

Types of instances in AWS:
- on-demand instance: 
	- Procure when you need them. You provision it based on your requirements and then shut it down or terminate it.
	- Pay for what you use on hourly basis.
	- If you shut down a VM (EC2 instance), you do not pay the "running charges".
	- An instance that has been shut down, can be restarted.
	- But an instance that has been terminated, cannot be restarted.
	- AWS EMR dos not have a "shut down" option. You can only terminate it.
- spot instances:
	- AWS has many instances that remain unused. Lot of free capacity.
	- AWS gives these to users at a highly discounted price. ~90% cheaper compared on-demand.
	- But, AWS can take it back whenever it wants at a 2-minute notice.
	- This is because the spot-instances are based on the "auction" strategy.
	- Used for non-critical application and when you have very low budget.
	- Use it wisely!
- Reserved instances:
	- Commit upfront on the usage of the instance(s).
	- AWS will give discounts for upfront payments and long term commitments.
	- It is cheaper compared to on-demand, provided you use it properly.
	- Mostly used for PRODUCTION environments.
	
Node Types on AWS EMR Spark Cluster:
------------------------------------
1. Master node:
	- Manages the cluster.
	- Single EC2 instance.
2. Core Node:
	- One or more core nodes.
	- Hosts HDFS data and also capable of running tasks (both storage and compute).
3. Task Node:
	- Only compute (you cannot use it for HDFS storage).
	- Can only be used for running tasks.
	- Have as many as you want.
	
Each node is an EC2 instance.

If your application is compute-heavy (requires a lot of processing, less storage), then you can go for more task nodes to adds more computing power to your cluster.
For Taks Nodes, you can also go for spot instances because even if an instance taken away, you are not losing any data.


The Cluster can be of two types:
- Transient cluster.
	- Will auto-terminate after all the steps in a job are completed.
	- a.k.a. Step Execution.
- Long running cluster.
	- when you are doing some R&D, continuous analysis etc.
	- Make sure you terminate the cluster when not in use to avoid recurring charges.
	
Scenario: A reporting Spark Job that needs to run at 12:00 pm daily.
	- Go for Transient cluster.
	- Do the job and terminate the cluster.
	
Creating an AWS EMR Cluster:
1. EC2 Key pair for SSH.
	- Create and download the .ppl file to be used in PuTTY to connect to the EC2 instance.
2. Create an AWS EMR Cluster.
	- 2 Launch modes:
		- Cluster (select this mode).
		- Step Execution.
	- When creating the cluster, status is "Starting".
	- When it is ready for use, status is "Waiting".
	- When terminated, status is "Terminated".

Run a Word Count Spark Job on the Long running cluster:
1. Create the package (jar file) of the scala project in sbt.
2. Create a S3 bucket and folder for the text file to be processed by the WordCount Spark Job.
3. Copy the text file to S3 in the bucket+folder created in step 2.
4. Create a S3 bucket (can use the same bucket) and folder for the jar file (package) created in step 1.
5. Copy the jar file from step 1 to the bucket+folder created in step 4.
6. Using PuTTY, connect to the server (master node of the cluster). 
	Hostname: hadoop@<public dns address of the master node> (The dns can be found on the EMR Cluster's dashboard).
	Port: 22
	In Connection -> SSH -> Auth: Select the .ppk file downloaded earlier (EC2 Key Pair file).
	Select Open.
7. It will fail initially because the port 22 is not open on the master node.
8. From AWS EC2 section, select the master node instance and then the "Security" tab in the bottom pane.
9. Scroll down to "Security Groups" in the Security tab and click on the link there.
10. Select the Inbound Rules tab.
11. Click on Edit Inbound Rules.
12. Scroll down and select Add Rule.
13. Select SSH, Port 22, My IP.
14. Click on Save Rules at the bottom.
15. Try PuTTY again on Step 6.
16. Success!
17. Copy the jar file from s3 to a folder on the instance by executing:
	aws s3 cp <url of the jar on s3> .
	For e.g.; aws s3 cp s3://ajshdoop/jars/wordcount-SNAPSHOT.jar .
18. Run the spark job as:
	spark-submit ./wordcount-SNAPSHOT.jar --class example.WordCount

Tomorrow:
Spark Cluster settings:
Memory
Executors
Caching

SparkSQL and DataFrames

